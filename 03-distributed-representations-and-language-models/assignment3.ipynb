{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3: Word Embeddings and Language Modelling\n",
    "\n",
    "Created by Adam Ek, modified by Ricardo Muñoz Sánchez and Simon Dobnik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "In this lab we will explore constructing *static* word embeddings (i.e. word2vec) and building language models. We'll also evaluate these systems on intermediate tasks, namely word similarity and identifying \"good\" and \"bad\" sentences.\n",
    "\n",
    "* For this we'll use pytorch.\n",
    "    * You can install it using the instructions from here: https://pytorch.org/\n",
    "    * If you would like to check out some tutorials on how to use it, you can can do so here: https://pytorch.org/tutorials/beginner/basics/intro.html\n",
    "    * Some basic operations that will be useful for you can be found here: https://jhui.github.io/2018/02/09/PyTorch-Basic-operations\n",
    "* We are not interested in getting state-of-the-art performance, focus on the implementation and not results of your model.\n",
    "    * For this reason, you can use a subset of the dataset: the first 5000-10 000 sentences or so.\n",
    "    * On linux or mac you can use: ```head -n 10000 inputfile > outputfile```. \n",
    "* Using GPUs will make things run faster.\n",
    "    * You can access the server by using SSH: ```ssh -L 8888:localhost:8888 [your_x_account]@mltgpu.flov.gu.se -p 62266```\n",
    "        * ```ssh``` tells the computer to connect remotely to the server.\n",
    "        * ```-L 8888:localhost:8888``` allows you to connect using jupyter notebooks, you can remove it if you don't want to do that.\n",
    "        * ```-p 62266``` tells the server to give you access through port 62266.\n",
    "    * You can also connect to the server using VSCode, available for Mac, Linux, and Windows.\n",
    "    * I would suggest you to set up a virtual environment on the server, such as virtual env or conda.\n",
    "    * When using pytorch on the server, remember to install the GPU-compatible version!\n",
    "    * You can also use Google Collab for free (with a monthly quota for GPU usage). We highly suggest you to use the MLT server instead, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# If you're using GPUs, replace \"cpu\" with \"cuda:n\" where n is the index of the GPU\n",
    "# I run the script on the MLT server\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look up available GPUs\n",
    "# print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "\n",
    "# for i in range(torch.cuda.device_count()):\n",
    "#     print(\"GPU index:\", i, \"Name:\", torch.cuda.get_device_name(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec embeddings\n",
    "\n",
    "In this first part we'll construct a word2vec model which will give us *static* word embeddings (that is, they are fixed after training).\n",
    "\n",
    "After we've trained our model we will evaluate the embeddings obtained on a word similarity task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load some data from data/wiki-corpus.50000.txt. The file contains 50 000 sentences randomly selected from the complete wikipedia. Each line in the file contains one sentence. The sentences are whitespace tokenized.\n",
    "\n",
    "Your first task is to create a dataset suitable for word2vec. That is, we define some ```window_size``` then iterate over all sentences in the dataset, putting the target word in one column and the context words in another (separate the columns with ```tab```). ```window_size=n``` means that we select ```n/2``` tokens to the right and left of the center word.\n",
    "\n",
    "For example, the sentece \"this is a lab and exercise\" with ```window size = 4``` will be converted to 6 (target, context) pairs:\n",
    "```\n",
    "target      context\n",
    "----------------------------\n",
    "this        is, a\n",
    "is          this, a, lab\n",
    "a           this, is, lab\n",
    "lab         is, a, and, exercise\n",
    "and         a, lab, exercise\n",
    "exercise    lab, and \n",
    "```\n",
    "\n",
    "this will be our training examples for the word2vec model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter # for filtering out uncommon pairs\n",
    "\n",
    "data_path = 'data/wiki-corpus.50000.txt'\n",
    "WINDOW_SIZE = 4\n",
    "def corpus_reader(data_path, window_size=4, min_freq=4):\n",
    "    all_data = []\n",
    "    vocabulary = set(['<pad>'])\n",
    "    with open(data_path, encoding = 'utf-8') as f:\n",
    "        # go over the lines (sentences in the files)\n",
    "        for line in f:\n",
    "            # split sentences into tokens\n",
    "            tokens = line.strip().split(' ')\n",
    "            # save all individual words to the vocabulary\n",
    "            for token in tokens:\n",
    "                vocabulary.add(token)\n",
    "            # extract all (center word, context) with `window_size=4`, pairs from the sentence\n",
    "            for i, center_word in enumerate(tokens):\n",
    "                context = []\n",
    "                for j in range(max(i - window_size//2, 0), min(i + window_size//2 + 1, len(tokens))):\n",
    "                    if i != j:\n",
    "                        context.append(tokens[j])\n",
    "                # save (center word, context) pairs into a dataset\n",
    "                all_data.append((center_word, context)) \n",
    "    \n",
    "    # filter out words which does not occur often\n",
    "    word_counts = Counter(word for pair in all_data for word, _ in [pair])\n",
    "    # print(word_counts)\n",
    "    \n",
    "    filtered_vocabulary = {word for word, count in word_counts.items() if count >= min_freq}\n",
    "    # print(\"check:\",word_counts['Woodcock'])\n",
    "    \n",
    "    # create a mapping from words to integers. \n",
    "    # each word should have an unique integer mapped to it. \n",
    "    # use a dictionary for this.\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(filtered_vocabulary)}\n",
    "    return all_data, word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "MIN_FREQ = 4 # may leads to KeyError in following indices mapping => deal with pre-judgement\n",
    "\n",
    "all_data, word_to_idx = corpus_reader(data_path, window_size=WINDOW_SIZE, min_freq=MIN_FREQ)\n",
    "# print(\"Word to index mapping:\", word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sampled 50 000 senteces completely random from the *whole* wikipedia for our training data. Give some reasons why this is good, and why it might be bad. (*note*: We'll have a few questions like these, one or two reasons for and against is sufficient)\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits**:\n",
    "The diversity ensured by randomness helps in training a robust model capable of generalizing across various text types, while also potentially saving computational resources and time due to its simplicity and efficiency.\n",
    "\n",
    "**Possible limitations**:\n",
    "Random sampling might not ensure a balanced representation of all topics or domains present in Wikipedia. Certain topics may be overrepresented or underrepresented in the sampled data, leading to biases in the trained model's understanding and predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "We need to create a dataloader now. That is, some way of generating a batch of examples from the dataset. A batch is a set of ```n``` examples from the data.\n",
    "\n",
    "The recipe for a dataloader is as follows:\n",
    "\n",
    "* Select n examples from the dataset\n",
    "* (a) Translate each example into integers using `word_to_idx`\n",
    "* (b) Transform the translated examples to pytorch tensors\n",
    "* (c) Return the batch \n",
    "* Select n new examples from the dataset\n",
    "* ... repeat steps (a-c)\n",
    "\n",
    "The dataloader should stop when it have read the whole dataset.\n",
    "\n",
    "This can be done either by first computing all the batches in the dataset and returning it as a list which you can then iterate over, or as an generator that returns each batch after it has been created.\n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx['<unk>'] = len(word_to_idx) # add '<unk>' to deal with unknown(uncommon) words => pay attention to dimension when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Batch = namedtuple('Batch', ['target_word', 'context'])\n",
    "\n",
    "def batcher(dataset, word_to_idx, batch_size=8):\n",
    "    batches = []\n",
    "    # iterate over the dataset\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch_data = dataset[i:i+batch_size]  # select a batch of size `batch_size`\n",
    "        batch_target_words, batch_contexts = zip(*batch_data) # *: unpacking\n",
    "\n",
    "        # translate batch to integers using `word_to_idx\n",
    "        \n",
    "        batch_target_word_indices = [word_to_idx[word] if word in word_to_idx else word_to_idx['<unk>'] for word in batch_target_words]\n",
    "        batch_context_indices = [[word_to_idx[word] if word in word_to_idx else word_to_idx['<unk>'] for word in context] for context in batch_contexts]\n",
    "\n",
    "    \n",
    "        # add padding to the context(unify the length of contexts)\n",
    "        max_context_length = max(len(context) for context in batch_context_indices)\n",
    "        padded_batch_contexts = [context + [0] * (max_context_length - len(context)) for context in batch_context_indices]\n",
    "   \n",
    "        # transform the batch to a pytorch tensor\n",
    "        tensor_batch_target_word = torch.tensor(batch_target_word_indices)\n",
    "        tensor_batch_context = torch.tensor(padded_batch_contexts)\n",
    "    \n",
    "        # return the dataset of batches/individual batches \n",
    "        batch = Batch(target_word=tensor_batch_target_word, context=tensor_batch_context)\n",
    "        batches.append(batch)\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lower-cased all tokens above; give some reasons why this is a good idea, and why it may be harmful to our embeddings.\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**:\n",
    "Lowercasing all tokens helps in standardizing the text data, reducing the complexity of the vocabulary and ensuring that identical words with different casing are treated as the same token. For example, \"Apple\" and \"apple\" would be treated as the same word after lowercasing.\n",
    "\n",
    "**Possible Harmness**:\n",
    "Lowercasing can lead to loss of information, especially in cases where the casing of words carries semantic meaning. For example, in named entity recognition tasks, capitalization often indicates proper nouns, which may be lost after lowercasing. Additionally, lowercasing can introduce ambiguity in some cases. For example, the word \"US\" could represent the United States or the pronoun \"us\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the CBOW model for constructing word embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the CBOW model we try to predict the center word based on the context. That is, we take as input ```n``` context words, encode them as vectors, then combine them by summation. This will give us one embedding. We then use this embedding to predict *which* word in our vocabuary is the most likely center word. \n",
    "\n",
    "Implement this model \n",
    "\n",
    "[7 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        # where the embeddings of words are stored \n",
    "        # each word in the vocabulary should have one embedding assigned to it\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # a transformation that predicts a word from the vocabulary\n",
    "        self.prediction = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, context):\n",
    "        # translate a batch to embeddings\n",
    "        # print(\"Context:\", context)\n",
    "\n",
    "        embedded_context = self.embeddings(context)  # (B, S, D) B - batch size, S - sequence length(window size), D - embedding dimension \n",
    "        # reduce dimensions of the embeddings\n",
    "        projection = self.projection_function(embedded_context) # (B, D)\n",
    "        # predict the target word from the vocabulary\n",
    "        predictions = self.prediction(projection)  # (B, vocab_size)\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    def projection_function(self, xs):\n",
    "        \"\"\"\n",
    "        This function will take as input a tensor of size (B, S, D)\n",
    "        where B is the batch_size, S the window size, and D the dimensionality of embeddings\n",
    "        this function should compute the sum over the embedding dimensions of the input, \n",
    "        that is, we transform (B, S, D) to (B, 1, D) or (B, D) \n",
    "        \"\"\"\n",
    "        xs_sum = torch.sum(xs, dim=1)  # sum over the window size dimension\n",
    "        return xs_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train the models. First we define which hyperparameters to use. (You can change these, for example when *developing* your model you can use a batch size of 2 and a very low dimensionality (say 10), just to speed things up). When actually training your model *fo real*, you can use a batch size of [8,16,32,64], and embedding dimensionality of [128,256]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_hyperparameters = {'epochs':10, # given 3\n",
    "                                   'batch_size':16,\n",
    "                                   'lr':0.001, # given 'learning_rate' here; to unify with code below => 'lr'\n",
    "                                   'embedding_dim':128}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model. Iterate over the dataset, get outputs from your model, calculate loss and backpropagate.\n",
    "\n",
    "We mentioned in the lecture that we use Negative Log Likelihood (https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) loss to train Word2Vec model. In this lab we'll take a shortcut when *training* and use Cross Entropy Loss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), basically it combines ```log_softmax``` and ```NLLLoss```. So what your model should output is a *score* for each word in our vocabulary. The ```CrossEntropyLoss``` will then assign probabilities and calculate the negative log likelihood loss.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "batches = batcher(all_data, word_to_idx, batch_size=word_embeddings_hyperparameters['batch_size']) # given dataset, vocab = get_data(..) here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, average loss:5.9515\n",
      "epoch 2, average loss:5.6893\n",
      "epoch 3, average loss:5.5378\n",
      "epoch 4, average loss:5.4228\n",
      "epoch 5, average loss:5.3306\n",
      "epoch 6, average loss:5.2581\n",
      "epoch 7, average loss:5.1986\n",
      "epoch 8, average loss:5.1504\n",
      "epoch 9, average loss:5.1101\n",
      "epoch 10, average loss:5.0769\n",
      "training finished!\n"
     ]
    }
   ],
   "source": [
    "# build model and construct loss/optimizer\n",
    "cbow_model = CBOWModel(len(word_to_idx), word_embeddings_hyperparameters['embedding_dim'])\n",
    "cbow_model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=word_embeddings_hyperparameters['lr'])\n",
    "\n",
    "# start training loop\n",
    "\n",
    "for epoch in range(word_embeddings_hyperparameters['epochs']):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(batches):\n",
    "        \n",
    "        context = batch.context # dim = 2, (16,4), i.e.,(batch_size,context_size)\n",
    "        target_word = batch.target_word # dim = 1\n",
    "\n",
    "        context = context.to(device)\n",
    "        target_word = target_word.to(device)\n",
    "        \n",
    "        # send your batch of sentences to the model\n",
    "        output = cbow_model(context)\n",
    "        \n",
    "        # compute the loss, you'll need to reshape the input\n",
    "        # you can read more about this is the documentation for\n",
    "        # CrossEntropyLoss\n",
    "        loss = loss_fn(output, target_word.view(-1)) # assuming target_word is a 1D tensor\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "      \n",
    "    # print average loss for the epoch\n",
    "    epoch_avg_loss = total_loss / len(batches)\n",
    "    print(f\"epoch {epoch+1}, average loss:{epoch_avg_loss:.4f}\")\n",
    "        \n",
    "print(\"training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the model on a dataset of word similarities, WordSim353 (http://alfonseca.org/eng/research/wordsim353.html , also avalable in the data folder). The first thing we need to do is read the dataset and translate it to integers. What we'll do is to reuse the ```Field``` that records word indexes (the second output of ```get_data()```) and use it to parse the file.\n",
    "\n",
    "The wordsim data is structured as follows:\n",
    "\n",
    "```\n",
    "word1 word2 score\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "The ```Field``` we got from ```read_data()``` has two built-in functions, ```stoi``` which maps a string to an integer and ```itos``` which maps an integer to a string. \n",
    "\n",
    "What our datareader needs to do is: \n",
    "\n",
    "```\n",
    "for line in file:\n",
    "    word1, word2, score = file.split()\n",
    "    # encode word1 and word2 as integers\n",
    "    word1_idx = vocab.vocab.stoi[word1]\n",
    "    word2_idx = vocab.vocab.stoi[word2]\n",
    "```\n",
    "\n",
    "when we have the integers for ```word_1``` and ```word2``` we'll compute the similarity between their word embeddings with *cosine simlarity*. We can obtain the embeddings by querying the embedding layer of the model.\n",
    "\n",
    "We calculate the cosine similarity for each word pair in the dataset, then compute the pearson correlation between the similarities we obtained with the scores given in the dataset. \n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Field:\n",
    "#     def __init__(self):\n",
    "#         self.stoi = {}\n",
    "#         self.itos = []\n",
    "\n",
    "# class Vocabulary:\n",
    "#     def __init__(self):\n",
    "#         self.vocab = Field()\n",
    "\n",
    "# def read_data(data_path):\n",
    "#     # Create a Vocabulary object\n",
    "#     vocab = Vocabulary()\n",
    "\n",
    "#     # Read data from data_path and process it\n",
    "#     data = set()\n",
    "#     with open(data_path, encoding = 'utf-8') as f:\n",
    "#         # go over the lines (sentences in the files)\n",
    "#         for line in f:\n",
    "#             # split sentences into tokens\n",
    "#             tokens = line.strip().split(' ')\n",
    "#             # save all individual words to the vocabulary\n",
    "#             for token in tokens:\n",
    "#                 data.add(token)\n",
    "\n",
    "#     # Populate stoi and itos\n",
    "#     for i, word in enumerate(list(data)):\n",
    "#         vocab.vocab.stoi[word] = i\n",
    "#         vocab.vocab.itos.append(word)\n",
    "\n",
    "#     return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = read_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = cbow_model.embeddings.weight.data\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.40454703]\n",
      " [0.40454703 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def read_wordsim(path, word_to_idx, embeddings): # def read_wordsim(path, vocab, embeddings): don't need vocab any more \n",
    "    dataset_sims = {} # for further analysis that which word pairs have better/worse prediction effects, involve word_pair as key\n",
    "    model_sims = {}\n",
    "    with open(path, encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            \n",
    "            word1, word2, score = line.split()\n",
    "            score = float(score)\n",
    "            \n",
    "            dataset_sims[word1, word2] = score\n",
    "            \n",
    "            model_sims[word1, word2] = None \n",
    "            # get the index for the word\n",
    "            if word1 in word_to_idx and word2 in word_to_idx:\n",
    "                word1_idx = word_to_idx[word1]\n",
    "                word2_idx = word_to_idx[word2]\n",
    "            \n",
    "                # get the embedding of the word\n",
    "                word1_emb = embeddings[word1_idx]\n",
    "                word2_emb = embeddings[word2_idx]\n",
    "            \n",
    "                # compute cosine similarity, we'll use the version included in pytorch functional\n",
    "                # https://pytorch.org/docs/master/generated/torch.nn.functional.cosine_similarity.html\n",
    "                cosine_similarity = F.cosine_similarity(word1_emb.unsqueeze(0), word2_emb.unsqueeze(0)) #  default dim = 1, i.e., the dimension along which cosine similarity is computed\n",
    "\n",
    "                model_sims[word1,word2] = cosine_similarity.item()\n",
    "\n",
    "    # print(\"dataset_sims:\",dataset_sims)\n",
    "    # print(\"model_sims\",model_sims)\n",
    "    \n",
    "    return dataset_sims, model_sims\n",
    "\n",
    "path = 'data/wordsim_similarity_goldstandard.txt'\n",
    "data, model = read_wordsim(path, word_to_idx, embeddings)\n",
    "# pearson_correlation = np.corrcoef(data, model)\n",
    "\n",
    "# deal with NAs\n",
    "cleaned_data = {}\n",
    "cleaned_model = {}\n",
    "for d,m in zip(data.items(), model.items()):\n",
    "    word_pair,d_sims = d\n",
    "    _,m_sims = m\n",
    "\n",
    "    if d_sims is not None and m_sims is not None:\n",
    "        cleaned_data[word_pair] = d_sims # range:0-10 not similar-similar\n",
    "        cleaned_model[word_pair] = m_sims\n",
    "        \n",
    "cleaned_data_values = np.array(list(cleaned_data.values()))\n",
    "cleaned_model_values = np.array(list(cleaned_model.values()))\n",
    "\n",
    "# print(\"cleaned_data_values:\",cleaned_data_values)\n",
    "# print(\"cleaned_model_values:\",cleaned_model_values)\n",
    "\n",
    "pearson_correlation = np.corrcoef(cleaned_data_values, cleaned_model_values)\n",
    "         \n",
    "# the non-diagonals give the pearson correlation\n",
    "print(pearson_correlation) \n",
    "# record of training results with different combinations of hyperparameters\n",
    "# 3 epochs, batch_size 16, embedding_dim 128, lr 0.001 - 0.3193\n",
    "# 30 epochs, batch_size 16, embedding_dim 128, lr 0.001 - 0.2941 ?? => overfitting: (adjust hyperparameters) epochs--, embedding_dim++, batch_size--, etc.\n",
    "# 10 epochs, batch_size 16, embedding_dim 128, lr 0.001 - 0.34+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find out NAs' prop\n",
    "# print(\"original:\",len(data),\"vs.\",\"cleaned:\",len(cleaned_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the model performs good or bad? Why?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation coefficient of *0.40* indicates that there is a certain degree of linear relationship between the model's predictions and actual values, but this relationship is not strong. Specifically, this value may mean that the model's similarity predictions are relatively accurate for some word pairs, but not accurate enough for other word pairs.\n",
    "\n",
    "Taken together, **the model's performance is not very good, but it is not very bad either**. There is room for improvement in the model. We can try to improve the performance of the model by adjusting the model architecture, hyperparameters, etc. In addition, we can also consider further analyzing which word pairs have better prediction effects and which word pairs have worse prediction effects, so as to better understand the performance of the model and the direction of improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the 10 best and 10 worst performing word pairs, can you see any patterns that explain why *these* are the best and worst word pairs?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 best performing word pairs:\n",
      "('car', 'flight')\n",
      "the absolute rank difference: 0\n",
      "model_sim: 0.11908101290464401 , dataset_sim: 4.94\n",
      "model_rank: 86 , dataset_rank: 86\n",
      "('morality', 'marriage')\n",
      "the absolute rank difference: 1\n",
      "model_sim: 0.053934745490550995 , dataset_sim: 3.69\n",
      "model_rank: 52 , dataset_rank: 51\n",
      "('start', 'year')\n",
      "the absolute rank difference: 1\n",
      "model_sim: 0.08203141391277313 , dataset_sim: 4.06\n",
      "model_rank: 65 , dataset_rank: 64\n",
      "('Harvard', 'Yale')\n",
      "the absolute rank difference: 1\n",
      "model_sim: 0.23355956375598907 , dataset_sim: 8.13\n",
      "model_rank: 131 , dataset_rank: 130\n",
      "('Wednesday', 'news')\n",
      "the absolute rank difference: 2\n",
      "model_sim: -0.00496915727853775 , dataset_sim: 2.22\n",
      "model_rank: 19 , dataset_rank: 17\n",
      "('coast', 'shore')\n",
      "the absolute rank difference: 2\n",
      "model_sim: 0.5439820289611816 , dataset_sim: 9.1\n",
      "model_rank: 144 , dataset_rank: 142\n",
      "('holy', 'sex')\n",
      "the absolute rank difference: 3\n",
      "model_sim: -0.020599916577339172 , dataset_sim: 1.62\n",
      "model_rank: 13 , dataset_rank: 10\n",
      "('stock', 'egg')\n",
      "the absolute rank difference: 3\n",
      "model_sim: -0.0521775558590889 , dataset_sim: 1.81\n",
      "model_rank: 9 , dataset_rank: 12\n",
      "('peace', 'insurance')\n",
      "the absolute rank difference: 3\n",
      "model_sim: 0.01583627611398697 , dataset_sim: 2.94\n",
      "model_rank: 26 , dataset_rank: 29\n",
      "('music', 'project')\n",
      "the absolute rank difference: 3\n",
      "model_sim: 0.04430059716105461 , dataset_sim: 3.63\n",
      "model_rank: 46 , dataset_rank: 49\n",
      "\n",
      "Top 10 worst performing word pairs:\n",
      "('planet', 'moon')\n",
      "the absolute rank difference: 94\n",
      "model_sim: 0.030398672446608543 , dataset_sim: 8.08\n",
      "model_rank: 35 , dataset_rank: 129\n",
      "('opera', 'performance')\n",
      "the absolute rank difference: 95\n",
      "model_sim: -0.016987279057502747 , dataset_sim: 6.88\n",
      "model_rank: 14 , dataset_rank: 109\n",
      "('month', 'hotel')\n",
      "the absolute rank difference: 97\n",
      "model_sim: 0.17381392419338226 , dataset_sim: 1.81\n",
      "model_rank: 110 , dataset_rank: 13\n",
      "('street', 'block')\n",
      "the absolute rank difference: 97\n",
      "model_sim: -0.037131935358047485 , dataset_sim: 6.88\n",
      "model_rank: 11 , dataset_rank: 108\n",
      "('cell', 'phone')\n",
      "the absolute rank difference: 99\n",
      "model_sim: 0.013774558901786804 , dataset_sim: 7.81\n",
      "model_rank: 25 , dataset_rank: 124\n",
      "('dividend', 'payment')\n",
      "the absolute rank difference: 105\n",
      "model_sim: -0.011732984334230423 , dataset_sim: 7.63\n",
      "model_rank: 16 , dataset_rank: 121\n",
      "('energy', 'secretary')\n",
      "the absolute rank difference: 106\n",
      "model_sim: 0.18847140669822693 , dataset_sim: 1.81\n",
      "model_rank: 120 , dataset_rank: 14\n",
      "('monk', 'slave')\n",
      "the absolute rank difference: 112\n",
      "model_sim: 0.18363241851329803 , dataset_sim: 0.92\n",
      "model_rank: 117 , dataset_rank: 5\n",
      "('aluminum', 'metal')\n",
      "the absolute rank difference: 117\n",
      "model_sim: -0.05281795188784599 , dataset_sim: 7.83\n",
      "model_rank: 8 , dataset_rank: 125\n",
      "('stock', 'phone')\n",
      "the absolute rank difference: 121\n",
      "model_sim: 0.22936385869979858 , dataset_sim: 1.62\n",
      "model_rank: 130 , dataset_rank: 9\n"
     ]
    }
   ],
   "source": [
    "## main logic: smaller the absolute rank difference is, better the word pair performed (why not value: range differ)\n",
    "# compute the ranks\n",
    "data_ranks = {pair: rank + 1 for rank, (pair, sim) in enumerate(sorted(cleaned_data.items(), key=lambda x: x[1]))}\n",
    "model_ranks = {pair: rank + 1 for rank, (pair, sim) in enumerate(sorted(cleaned_model.items(), key=lambda x: x[1]))}\n",
    "\n",
    "# generate a sorted dict\n",
    "rank_diffs = {}\n",
    "for word_pair in data_ranks:\n",
    "    data_rank = data_ranks[word_pair]\n",
    "    model_rank = model_ranks[word_pair]\n",
    "    rank_diffs[word_pair] = abs(data_rank - model_rank)\n",
    "\n",
    "sorted_rank_diffs = sorted(rank_diffs.items(), key=lambda x: x[1])\n",
    "sorted_rank_diffs = dict(sorted_rank_diffs)\n",
    "# print(\"rank_diffs:\",rank_diffs)\n",
    "# print(\"sorted_rank_diffs:\",sorted_rank_diffs)\n",
    "\n",
    "best_pairs = list(sorted_rank_diffs)[:10]\n",
    "worst_pairs = list(sorted_rank_diffs)[-10:]\n",
    "\n",
    "# print\n",
    "print(\"Top 10 best performing word pairs:\")\n",
    "for pair in best_pairs:\n",
    "    print(pair)\n",
    "    print(\"the absolute rank difference:\",dict(sorted_rank_diffs)[pair])\n",
    "    print(\"model_sim:\",cleaned_model[pair],\",\",\"dataset_sim:\",cleaned_data[pair])\n",
    "    print(\"model_rank:\",model_ranks[pair],\",\",\"dataset_rank:\",data_ranks[pair])\n",
    "\n",
    "print(\"\\nTop 10 worst performing word pairs:\")\n",
    "for pair in worst_pairs:\n",
    "    print(pair)\n",
    "    print(\"the absolute rank difference:\",dict(sorted_rank_diffs)[pair])\n",
    "    print(\"model_sim:\",cleaned_model[pair],\",\",\"dataset_sim:\",cleaned_data[pair])\n",
    "    print(\"model_rank:\",model_ranks[pair],\",\",\"dataset_rank:\",data_ranks[pair])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, we could find that the trained word embeddings from CBOWmodel wrongly predict similar word pairs as dissimilar, e.g., ('planet', 'moon'), and predict dissimilar words with a relatively large similarity value, e.g., ('stock', 'phone').\n",
    "It might relate to the training corpora and training methods (hyperparameters like window_size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways of improving the model we apply to WordSim353.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I try to understand the *model* here, and I think it means improving the performance of any machine learning model used to predict the similarity of word pairs. This may involve:\n",
    "\n",
    "**Fine-tuning Pretrained Word Embeddings**: Pretrained word embeddings like *Word2Vec*, *GloVe*, or *fastText* can be fine-tuned on a dataset that is more specific to our task, such as a corpus similar to the one used in WordSim353. Fine-tuning allows the embeddings to capture *domain-specific* semantics better.\n",
    "\n",
    "**Model Architecture Tuning**: We can experiment with different neural network architectures such as *LSTM*, *Transformer* or *BERT* for learning word embeddings. Within the network, the *embedding_dim*, *number of layers* and *output_size* are all adjustable.\n",
    "\n",
    "**Hyperparameters Tuning**: Optimizing hyperparameters like *batch_size*, *learning_rate* and *epochs* can significantly impact the performance of the model. We can use grid search or random search to find the optimal combination of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider a scenario where we use these embeddings in a downstream task, for example sentiment analysis (roughly: determining whether a sentence is positive or negative). \n",
    "\n",
    "Give some examples why the sentiment analysis model would benefit from our embeddnings and one examples why our embeddings could hurt the performance of the sentiment model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits**:\n",
    "Word embeddings capture semantic information about words, including their meanings and relationships with other words. Wording embeddings can help the sentiment analysis model understand the context and meaning of words in a sentence, leading to more accurate sentiment predictions. For example, with dynamic word embeds, we can infer the exact POS and meaning of a *polysemy* in a specific context, such as \"light\". Additionally, its generalization ability enables the sentiment analysis model to perform effectively on sentences containing words that were not present in the training data.\n",
    "\n",
    "**Potential hurt**:\n",
    "Word embeddings can inherit biases present in the training data, which may adversely affect the sentiment analysis model's predictions. For example, if the word embeddings encode gender or racial biases, these biases may influence the sentiment analysis model's predictions in unintended ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part we'll build a simple LSTM language model. Your task is to construct a model which takes a sentence as input and predicts the next word for each word in the sentence. For this you'll use the ```LSTM``` class provided by PyTorch (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). You can read more about the LSTM here: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "NOTE!!!: Use the same dataset (wiki-corpus.50000.txt) as before.\n",
    "\n",
    "Our setup is similar to before, we first encode the words as distributed representations then pass these to the LSTM and for each output we predict the next word.\n",
    "\n",
    "To create a gold standard (what we want to predict), we need to manipulate the tensor containing the sentence. As wi want to predict the *next* word, we want the following setup (where `w_n` is the index of a word in the sentence, `x` is the input words, and `y` is the gold words):\n",
    "\n",
    "$x = [w_0, w_1, w_2, w_3, w_4]$\n",
    "\n",
    "$y = [w_1, w_2, w_3, w_4, w_5]$\n",
    "\n",
    "That is, to create the gold standard we need to shift the index `n` of the input by `+1`, as this gives us the next word.\n",
    "\n",
    "\n",
    "For this we'll build a new dataloader, the file we pass to the dataloader should contain one sentence per line, with words separated by whitespace.\n",
    "\n",
    "```\n",
    "word_1, ..., word_n\n",
    "word_1, ..., word_k\n",
    "...\n",
    "```\n",
    "\n",
    "in this dataloader you want to make sure that each sentence begins with a ```<start>``` token and ends with a ```<end>``` token. But other than that, just as before you read the dataset and output an iterator over the dataset, a vocabulary, and a mapping from words to indices. \n",
    "\n",
    "Implement the dataloader, language model and the training loop (the training loop will basically be the same as for word2vec).\n",
    "\n",
    "[12 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs as before\n",
    "lm_hyperparameters = {'epochs':10,\n",
    "                      'batch_size':1, # pass one sent each time\n",
    "                      'lr':0.001, # given 'learning_rate' here; to unify with code below => 'lr'\n",
    "                      'embedding_dim':300,\n",
    "                      'hidden_dim':128} # given 'output_dim' here, but output_size usually equals to vocal_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm_Batch = namedtuple('Batch', 'sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/wiki-corpus.50000.txt'\n",
    "\n",
    "def get_data(path, batch_size = 16, min_freq = 4):\n",
    "    # your code here, roughly the same as for the word2vec dataloader\n",
    "    all_sents = []\n",
    "    vocab = []\n",
    "    with open(data_path, encoding = 'utf-8') as f:\n",
    "        # go over the lines (sentences in the files)\n",
    "        line_count = 0\n",
    "        for line in f:\n",
    "            # report out of memory, try to select a subset(first 5000/10000 sents)\n",
    "            if line_count >= 10000: # 5000\n",
    "                break\n",
    "           \n",
    "            # split sentences into tokens\n",
    "            tokens = line.strip().split(' ')\n",
    "            \n",
    "            # insert '<start>' and '<end>' respectively to compose a standard token list\n",
    "            tokens_std = ['<start>']\n",
    "            tokens_std.extend(tokens)\n",
    "            tokens_std.append('<end>')\n",
    "            for token in tokens_std:\n",
    "                vocab.append(token)\n",
    "\n",
    "            all_sents.append(tokens_std) \n",
    "\n",
    "            line_count += 1\n",
    "\n",
    "    # print(\"vocab:\",vocab)\n",
    "        \n",
    "    # filter out words which does not occur often\n",
    "    word_counts = Counter(word for word in vocab)\n",
    "    \n",
    "    filtered_vocab = {word for word, count in word_counts.items() if count >= min_freq}\n",
    "    # print(\"filtered_vocab:\",filtered_vocab)\n",
    "\n",
    "    # create a mapping from words to integers. \n",
    "    # each word should have an unique integer mapped to it. \n",
    "    # use a dictionary for this.\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(filtered_vocab)}\n",
    "\n",
    "    # Convert sentences to indices\n",
    "    all_sents_idx = []\n",
    "    for sent in all_sents:\n",
    "        sent_idx = [word_to_idx[word] for word in sent if word in word_to_idx]\n",
    "        all_sents_idx.append(sent_idx)\n",
    "\n",
    "    # Generate batches\n",
    "    batches = []\n",
    "    PAD_IDX = len(word_to_idx)\n",
    "    for i in range(0, len(all_sents_idx), batch_size):\n",
    "        batch = all_sents_idx[i:i+batch_size]\n",
    "        max_len = max(len(sent) for sent in batch)\n",
    "        padded_batch = [sent + [PAD_IDX] * (max_len - len(sent)) for sent in batch]\n",
    "        batches.append(lm_Batch(sentence=torch.tensor(padded_batch)))\n",
    "    \n",
    "    return batches, word_to_idx\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset, vocab = get_data(data_path, batch_size = lm_hyperparameters['batch_size'], min_freq = MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['<unk>'] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "class LM_withLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LM_withLSTM, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim)\n",
    "        self.predict_word = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        # extract embeddings for the sentence\n",
    "        embedded_seq = self.embeddings(seq)\n",
    "        # compute contextual representations\n",
    "        timestep_representation, *_ = self.LSTM(embedded_seq)\n",
    "        # predict a token from the vocabulary at each timestep\n",
    "        predicted_words = self.predict_word(timestep_representation)\n",
    "        \n",
    "        return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check memory before training\n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! ps -aux | grep python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, average loss:5.8708\n",
      "epoch 2, average loss:5.3027\n",
      "epoch 3, average loss:5.0784\n",
      "epoch 4, average loss:4.9229\n",
      "epoch 5, average loss:4.8085\n",
      "epoch 6, average loss:4.7223\n",
      "epoch 7, average loss:4.6595\n",
      "epoch 8, average loss:4.6125\n",
      "epoch 9, average loss:4.5809\n",
      "epoch 10, average loss:4.5561\n",
      "training finished!\n"
     ]
    }
   ],
   "source": [
    "# build model and construct loss/optimizer\n",
    "lm_model = LM_withLSTM(len(vocab), \n",
    "                       lm_hyperparameters['embedding_dim'],\n",
    "                       lm_hyperparameters['hidden_dim'])\n",
    "lm_model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lm_model.parameters(), lr=lm_hyperparameters['lr'])\n",
    "\n",
    "# start training loop\n",
    "for epoch in range(lm_hyperparameters['epochs']):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(dataset):\n",
    "        \n",
    "        # the strucure for each BATCH is:\n",
    "        # <start>, w0, ..., wn, <end>\n",
    "        sentence = batch.sentence\n",
    "        sentence = sentence.to(device)\n",
    "        \n",
    "        # when training the model, at each input we predict the *NEXT* token\n",
    "        # consequently there is nothing to predict when we give the model \n",
    "        # <end> as input. \n",
    "        # thus, we do not want to give <end> as input to the model, select \n",
    "        # from each batch all tokens except the last. \n",
    "        # tip: use pytorch indexing/slicing (same as numpy) \n",
    "        # (https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html#operations-on-tensors)\n",
    "        # (https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/)\n",
    "        input_sentence = sentence[:, :-1] \n",
    "\n",
    "        # send your batch of sentences to the model\n",
    "        output = lm_model(input_sentence)\n",
    "\n",
    "        # for each output, the model predict the NEXT token, so we have to reshape \n",
    "        # our dataset again. On timestep t, we evaluate on token t+1. That is,\n",
    "        # we never predict the <start> token ;) so this time, we select all but the first \n",
    "        # token from sentences (that is, all the tokens that we predict)\n",
    "        gold_data = sentence[:, 1:]\n",
    "\n",
    "        # the shape of the output and sentence variable need to be changed,\n",
    "        # for the loss function. Details are in the documentation.\n",
    "        # You can use .view(...,...) to reshape the tensors  \n",
    "        output = output.view(-1, len(vocab))\n",
    "        gold_data = gold_data.view(-1)\n",
    "\n",
    "        loss = loss_fn(output, gold_data)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # # print average loss for the epoch\n",
    "        # print(total_loss/(i+1), end='\\r') \n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # print average loss for the epoch\n",
    "    epoch_avg_loss = total_loss / len(dataset)\n",
    "    print(f\"epoch {epoch+1}, average loss:{epoch_avg_loss:.4f}\")\n",
    "    \n",
    "print(\"training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check compatibility\n",
    "#!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the language model\n",
    "\n",
    "We'll evaluate our model using the BLiMP dataset (https://github.com/alexwarstadt/blimp). The BLiMP dataset contains sets of linguistic minimal pairs for various syntactic and semantic phenomena, We'll evaluate our model on *existential quantifiers* (link: https://github.com/alexwarstadt/blimp/blob/master/data/existential_there_quantifiers_1.jsonl). This data, as the name suggests, investigate whether language models assign higher probability to *correct* usage of there-quantifiers. \n",
    "\n",
    "An example entry in the dataset is: \n",
    "\n",
    "```\n",
    "{\"sentence_good\": \"There was a documentary about music irritating Allison.\", \"sentence_bad\": \"There was each documentary about music irritating Allison.\", \"field\": \"semantics\", \"linguistics_term\": \"quantifiers\", \"UID\": \"existential_there_quantifiers_1\", \"simple_LM_method\": true, \"one_prefix_method\": false, \"two_prefix_method\": false, \"lexically_identical\": false, \"pairID\": \"0\"}\n",
    "```\n",
    "\n",
    "Download the dataset and build a datareader (similar to what you did for word2vec). The dataset structure you should aim for is (you don't need to worry about the other keys for this assignment):\n",
    "\n",
    "```\n",
    "good_sentence_1, bad_sentence_1\n",
    "...\n",
    "```\n",
    "\n",
    "your task now is to compare the probability assigned to the good sentence with to the probability assigned to the bad sentence. To compute a probability for a sentence we consider the product of the probabilities assigned to the *gold* tokens, remember, at timestep ```t``` we're predicting which token comes *next* e.g. ```t+1``` (basically, you do the same thing as you did when training).\n",
    "\n",
    "In rough pseudo code what your code should do is:\n",
    "\n",
    "```\n",
    "accuracy = []\n",
    "for good_sentence, bad_sentence in dataset:\n",
    "    gs_lm_output = LanguageModel(good_sentence)\n",
    "    gs_token_probabilities = softmax(gs_lm_output)\n",
    "    gs_sentence_probability = product(gs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    bs_lm_output = LanguageModel(bad_sentence)\n",
    "    bs_token_probabilities = softmax(bs_lm_output)\n",
    "    bs_sentence_probability = product(bs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    # int(True) = 1 and int(False) = 0\n",
    "    is_correct = int(gs_sentence_probability > bs_sentence_probability)\n",
    "    accuracy.append(is_correct)\n",
    "\n",
    "print(numpy.mean(accuracy))\n",
    "    \n",
    "```\n",
    "\n",
    "[6 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy:\n",
      "0.63\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "import json\n",
    "\n",
    "def evaluate_model(path, vocab, model):\n",
    "\n",
    "    # threshold_value = 0.5\n",
    "    accuracy = []\n",
    "    with open(path) as f:\n",
    "        # iterate over one pair of sentences at a time\n",
    "        for line in f:\n",
    "            # load the data\n",
    "            data = json.loads(line)\n",
    "            good_s = data['sentence_good']\n",
    "            bad_s = data['sentence_bad']\n",
    "            \n",
    "            # the data is tokenized as whitespace\n",
    "            tok_good_s = good_s.split()\n",
    "            tok_bad_s = bad_s.split()\n",
    "            \n",
    "            # encode your words as integers using the vocab from the dataloader, size is (S)\n",
    "            # we use unsqueeze to create the batch dimension \n",
    "            # in this case our input is only ONE batch, so the size of the tensor becomes: \n",
    "            # (S) -> (1, S) as the model expects batches\n",
    "            default_idx = len(vocab)-1\n",
    "            enc_good_s = torch.tensor([vocab.get(word, default_idx) for word in tok_good_s]).unsqueeze(0)\n",
    "            enc_bad_s = torch.tensor([vocab.get(word, default_idx) for word in tok_bad_s]).unsqueeze(0)\n",
    "            \n",
    "            enc_good_s = enc_good_s.to(device)\n",
    "            enc_bad_s = enc_bad_s.to(device)\n",
    "            \n",
    "            # print(\"enc_good_s:\",enc_good_s)\n",
    "            \n",
    "            # pass your encoded sentences to the model and predict the next tokens\n",
    "            good_s = model(enc_good_s)\n",
    "            bad_s = model(enc_bad_s)\n",
    "            \n",
    "            # get probabilities with softmax\n",
    "            gs_probs = F.softmax(good_s, dim=2) # pay attention to the normalization dim \n",
    "            bs_probs = F.softmax(bad_s, dim=2)\n",
    "            \n",
    "            # print(gs_probs.shape) # torch.Size([1, 8, 70])\n",
    "            # print(gs_probs[0, 0, 65].item())\n",
    "\n",
    "            # select the probability of the gold tokens\n",
    "            gs_sent_prob = find_token_probs(gs_probs, enc_good_s)\n",
    "            bs_sent_prob = find_token_probs(bs_probs, enc_bad_s)\n",
    "            \n",
    "            accuracy.append(int(gs_sent_prob>bs_sent_prob))\n",
    "            # accuracy.append(int(gs_sent_prob>threshold_value))\n",
    "            \n",
    "    return accuracy\n",
    "            \n",
    "def find_token_probs(model_probs, encoded_sentence):\n",
    "    probs = []\n",
    "\n",
    "    # iterate over the tokens in your encoded sentence\n",
    "    for token, gold_token in enumerate(encoded_sentence[0]):\n",
    "        # select the probability of the gold tokens and save\n",
    "        # hint: pytorch indexing is helpful here ;)\n",
    "        prob = model_probs[0, token, gold_token].item() # the probability of the gold token at the token-th position\n",
    "        probs.append(prob)\n",
    "    sentence_prob = np.prod(probs) # np.prod represents the probability of the entire sentence\n",
    "\n",
    "    return sentence_prob\n",
    "\n",
    "path = 'data/existential_there_quantifiers_1.jsonl'\n",
    "accuracy = evaluate_model(path, vocab, lm_model)\n",
    "\n",
    "print('Final accuracy:')\n",
    "print(np.round(np.mean(accuracy), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model get some score, say, 55% correct predictions. Is this good? Suggest some *baseline* (i.e. a stupid \"model\" we hope ours is better than) we can compare the model against.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common baseline model for binary classification tasks is a **random guessing baseline**:\n",
    "\n",
    "This baseline randomly predicts the class label for each instance in the dataset. For a binary classification task, where we predict whether a sentence pair aligns to the good-bad tag or not, we randomly assign each instance a class label (e.g., 0 or 1) with equal probability/a judgement rule.\n",
    "\n",
    "For accuracy used here, it represents an intuitive logic that *gs_sentence_probability > bs_sentence_probability*; to set a baseline model, we could set a *threshold_value* based on experience: **if gs_sentence_probability > threshold_value, it is considered to be correctly classified; otherwise, it is incorrectly classified**. For the sentence_probability is a product of all tokens, the threshold_value must be small (and dependent on the length of the sentence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some improvements you could make to your language model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Firstly**, limited to the server memory, we just select 5000/10000 lines to train our language model; if we enlarge the training corpora, we think the generalization effect would be better.\n",
    "\n",
    "**Secondly**, we can search through different hyperparameter combinations (learning rate, batch size, optimizer, etc.) to find the optimal settings for our language model.\n",
    "\n",
    "**Thirdly**, we can experiment with increasing/decreasing the size of our model (number of layers, hidden units, etc.), or trying other architectures such as Transformer, to see how it affects performance to find the one that performs best for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some other metrics we can use to evaluate our system\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used BLiMP dataset of good and bad sentences to evaluate language models. But we can also focus on other tasks such as text classification, text generation, etc.\n",
    "\n",
    "We can use some general metrics to evaluate the model such as **perplexity**, **F1 score**, etc.\n",
    "\n",
    "For example, the perplexity is a measure of how well a language model predicts a sample. F1 score reflects the balance between precision and recall, which means the model's ability to identify positive samples correctly.\n",
    "\n",
    "We can also use some specific metrics for language models such as BLEU, METEOR, ROUGE, CIDEr, SPICE, etc.\n",
    "\n",
    "For example, **BLEU** is a metric commonly used in translation tasks, that measures the similarity between the generated text and the reference text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature\n",
    "\n",
    "\n",
    "Neural architectures:\n",
    "\n",
    "[1] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. (Links to an external site.) Journal of Machine Learning Research, 3(6):1137–1155, 2003. (Sections 3 and 4 are less relevant today and hence you can glance through them quickly. Instead, look at the Mikolov papers where they describe training word embeddings with the current neural network architectures.)\n",
    "\n",
    "[2] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n",
    "\n",
    "[3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contribution**: Xiumei has written most of the code and answers for this assignment, Tianyi was involved in filling details, correction and discussion, Yiyi and Xi have been involved digitally.\n",
    "\n",
    "**Questions**: \n",
    "- How to select *batch_size* for the second LSTM model? \n",
    "\n",
    "  Actually, we first select common value like *16*, but for this model, we predict the next token in a sentence each time, so it seems natural to involve one sentence per batch for training. Are there other better choices of batch sizes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks\n",
    "\n",
    "The assignment is marked on a 7-level scale where 4 is sufficient to complete the assignment; 5 is good solid work; 6 is excellent work, covers most of the assignment; and 7: creative work. \n",
    "\n",
    "This assignment has a total of 63 marks. These translate to grades as follows: 1 = 17% 2 = 34%, 3 = 50%, 4 = 67%, 5 = 75%, 6 = 84%, 7 = 92% where %s are interpreted as lower bounds to achieve that grade."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
