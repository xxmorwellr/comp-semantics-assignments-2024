{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A5: Natural Language Inference using Neural Networks\n",
    "\n",
    "by Adam Ek, Bill Noble, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "\n",
    "In this lab we will work with neural networks for natural language inference. Our task is: given a premise sentence P and hypothesis H, what entailment relationship holds between them? Is H entailed by P, contradicted by P or neutral towards P?\n",
    "\n",
    "Given a sentence P, if H definitely describe something true given P then it is an **entailment**. If H describe something that's *maybe* true given P, it's **neutral**, and if H describe something that's definitely *false* given P it's a **contradiction**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore natural language inference using neural networks on the SNLI dataset, described in [1]. The dataset can be downloaded [here](https://nlp.stanford.edu/projects/snli/), but unfortunately that dataset is down at the moment. Instead, we will use the version uploaded to [HuggingFace](https://huggingface.co/datasets/stanfordnlp/snli) available through the `datasets` library. See the [documentation](https://huggingface.co/docs/datasets/v2.19.0/loading#hugging-face-hub) for loading a dataset from the HuggingFace hub.\n",
    "\n",
    "The (simplified) data is organized as follows:\n",
    "\n",
    "* Column 1: Premise\n",
    "* Column 2: Hypothesis\n",
    "* Column 3: Relation\n",
    "\n",
    "Like in the previous lab, we'll need to build a dataloader. You can adapt your code from the previous lab to the new dataset. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda:0')\n",
    "## MLT gpu is down this week, so we switch to pc to complete the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('snli')\n",
    "\n",
    "## filter out data with label `-1`\n",
    "def filter_labels(example):\n",
    "    return example['label'] != -1\n",
    "\n",
    "filtered_dataset = dataset.filter(filter_labels)\n",
    "# ex = dataset['train'][0]\n",
    "# print(dataset)\n",
    "# print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =  filtered_dataset['train']\n",
    "# validation = dataset['validation']\n",
    "test = filtered_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# print(\"summary labels:\",Counter(train['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dataset comes as a dictionary-like object with three splits: `'test'`, `'train'`, and `'validation'`. Each item is a dictionary containing a `'premise'`, `'hypothesis'`, and `'label'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "This data does not come pre-tokenized. Instead of training our own tokenizer, we can use the BERT tokenizer like in the preivous assignment. Even though we aren't using BERT the tokenizer works with any model. See the documentation on [using a pretrained tokenizer](https://huggingface.co/docs/tokenizers/en/quicktour#using-a-pretrained-tokenizer). **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# initialize tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# print(tokenizer.encode(ex['premise']).ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.encode(train[0]['premise']))\n",
    "# output: Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.utils.data as data_utils\n",
    "\n",
    "# merged_data = data_utils.ConcatDataset([train, test])\n",
    "\n",
    "# ## for padding, to computate the max_padding_length\n",
    "# premise_padding_length =  max(len(example['premise']) for example in merged_data)\n",
    "# hypothesis_padding_length =  max(len(example['hypothesis']) for example in merged_data)\n",
    "# print(premise_padding_length,hypothesis_padding_length) # 402 295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clarify\n",
    "# longest_premise_example = max(merged_data, key=lambda x: len(x['premise']))\n",
    "\n",
    "# print(\"Longest premise text:\")\n",
    "# print(longest_premise_example['premise'])\n",
    "\n",
    "# ## Why max_length so large => A sentence is treated as one long word, and each character is treated as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define encode function\n",
    "def encoded_data(data, tokenizer): \n",
    "    processed_data = []\n",
    "    for pair in data:\n",
    "        premise_encoded = tokenizer.encode(pair['premise']).ids\n",
    "        hypothesis_encoded = tokenizer.encode(pair['hypothesis']).ids\n",
    "\n",
    "        processed_data.append({'premise': premise_encoded, \n",
    "                             'hypothesis': hypothesis_encoded, \n",
    "                             'label': pair['label']})\n",
    "    return processed_data\n",
    "\n",
    "train_encoded = encoded_data(train, tokenizer)\n",
    "# validation_encoded = encoded_data(validation)\n",
    "test_encoded = encoded_data(test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.token_to_id(\"[PAD]\") # padding_value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \n",
    "    premises = [torch.tensor(item['premise'], dtype=torch.long) for item in batch]\n",
    "    hypothesis = [torch.tensor(item['hypothesis'], dtype=torch.long) for item in batch]\n",
    "    labels = [torch.tensor(item['label'], dtype=torch.long) for item in batch] # extract \n",
    "    \n",
    "    premises_padded = pad_sequence(premises, batch_first=True, padding_value=0)\n",
    "    hypothesis_padded = pad_sequence(hypothesis, batch_first=True, padding_value=0)  # padding\n",
    "\n",
    "    # convert to tensors\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        'premise': premises_padded,\n",
    "        'hypothesis': hypothesis_padded,\n",
    "        'label': labels_tensor\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_encoded[0]\n",
    "# label: 0-entailment, 1-neutral, 2-contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_encoded, batch_size = batch_size, shuffle = True, collate_fn = collate_fn)\n",
    "test_loader = DataLoader(test_encoded, batch_size = batch_size, shuffle = False, collate_fn = collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter = iter(train_loader)\n",
    "# train_batch = next(train_iter)\n",
    "\n",
    "# print(\"train_loader sample:\",train_batch) # the first batch\n",
    "# print(\"Shape of 'premise':\",train_batch['premise'].shape) \n",
    "# print(\"Shape of 'hypothesis':\",train_batch['hypothesis'].shape) # [batch_size, max_length(num_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll build the model for predicting the relationship between H and P.\n",
    "\n",
    "We will process each sentence using an LSTM. Then, we will construct some representation of the sentence. When we have a representation for H and P, we will combine them into one vector which we can use to predict the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model described in [2], the BiLSTM with max-pooling model. The procedure for the model is roughly:\n",
    "\n",
    "    1) Encode the Hypothesis and the Premise using one shared bidirectional LSTM (or two different LSTMS)\n",
    "    2) Perform max over the tokens in the premise and the hypothesis\n",
    "    3) Combine the encoded premise and encoded hypothesis into one representation\n",
    "    4) Predict the relationship "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a representation of a sentence\n",
    "\n",
    "Let's first consider step 2 where we perform pooling. There is a builtin function in pytorch for this, but we'll implement it from scratch. \n",
    "\n",
    "Let's consider the general case, what we want to do for these methods is apply some function $f$ along dimension $i$, and we want to do this for all $i$'s. As an example we consider the matrix S with size ``(N, D)`` where N is the number of words and D the number of dimensions:\n",
    "\n",
    "$S = \\begin{bmatrix}\n",
    "    s_{11} & s_{12} & s_{13} & \\dots  & s_{1d} \\\\\n",
    "    s_{21} & s_{22} & s_{23} & \\dots  & s_{2d} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    s_{n1} & s_{n2} & s_{n3} & \\dots  & s_{nd}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "What we want to do is apply our function $f$ on each dimension, taking the input $s_{1d}, s_{2d}, ..., s_{nd}$ and generating the output $x_d$. \n",
    "\n",
    "You will implement the max pooling method. When performing max-pooling, $max$ will be the function which selects a _maximum_ value from a vector and $x$ is the output, thus for each dimension $d$ in our output $x$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    x_d = max(s_{1d}, s_{2d}, ..., s_{nd})\n",
    "\\end{equation}\n",
    "\n",
    "This operation will reduce a batch of size ``(batch_size, num_words, dimensions)`` to ``(batch_size, dimensions)`` meaning that we now have created a sentence representation based on the content of the representation at each token position. \n",
    "\n",
    "Create a function that takes as input a tensor of size ``(batch_size, num_words, dimensions)`` then performs max pooling and returns the result (the output should be of size: ```(batch_size, dimensions)```). [**4 Marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(input_tensor):\n",
    "    output_tensor,_ = torch.max(input_tensor, dim=1) # return a tuple: maximum value and corresponding index\n",
    "    return output_tensor\n",
    "\n",
    "# test_unpooled = torch.rand(32, 100, 512)\n",
    "# test_pooled = max_pooling(test_unpooled)\n",
    "# print(test_pooled.size()) # should be torch.Size([32, 512]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining sentence representations\n",
    "\n",
    "Next, we need to combine the premise and hypothesis into one representation. We will do this by concatenating four tensors (the final size of our tensor $X$ should be ``(batch_size, 4d)`` where ``d`` is the number of dimensions that you use): \n",
    "\n",
    "$$X = [P; H; |P-H|; P \\cdot H]$$\n",
    "\n",
    "Here, what we do is concatenating P, H, P times H, and the absolute value of P minus H, then return the result.\n",
    "\n",
    "Implement the function. **[4 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_premise_and_hypothesis(hypothesis, premise):\n",
    "    # Concatenate hypothesis, premise, element-wise product, and absolute difference\n",
    "    output = torch.cat([\n",
    "        hypothesis,\n",
    "        premise,\n",
    "        hypothesis * premise,\n",
    "        torch.abs(hypothesis - premise)\n",
    "    ], dim=1)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# test_hypothesis = test_pooled.clone()\n",
    "# test_premise = test_pooled.clone()\n",
    "# test_combined = combine_premise_and_hypothesis(test_hypothesis, test_premise)\n",
    "# print(test_combined.size()) # should be torch.Size([32, 400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "Finally, we can build the model according to the procedure given previously by using the functions we defined above. Additionaly, in the model you should use *dropout*. For efficiency purposes, it's acceptable to only train the model with either max or mean pooling. \n",
    "\n",
    "Implement the model [**8 marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes, dropout_prob):\n",
    "        # your code goes here\n",
    "        super(SNLIModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 8, hidden_size),  # Concatenate premise and hypothesis representations\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, premise, hypothesis):\n",
    "        # Compute embeddings for premise and hypothesis\n",
    "        p_embedded = self.embeddings(premise)\n",
    "        h_embedded = self.embeddings(hypothesis)\n",
    "        # print(\"Shape of p_embedded\",p_embedded.shape) # (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "        # Pass through RNN\n",
    "        p_rnn_output, (_, _) = self.rnn(p_embedded)\n",
    "        h_rnn_output, (_, _) = self.rnn(h_embedded) # extract all hidden states\n",
    "\n",
    "        # print(\"Shape of p_rnn_output\",p_rnn_output.shape) # (batch_size, sequence_length, hidden_size*2)\n",
    "\n",
    "        # Perform pooling (max pooling)\n",
    "        p_pooled = max_pooling(p_rnn_output)  \n",
    "        h_pooled = max_pooling(h_rnn_output)\n",
    "\n",
    "        # Combine premise and hypothesis representations\n",
    "        ph_representation = combine_premise_and_hypothesis(p_pooled, h_pooled)\n",
    "\n",
    "        # Pass through classifier\n",
    "        predictions = self.classifier(ph_representation)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dir(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size() # 30522"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, implement the training and testing of the model. SNLI can take a very long time to train, so I suggest you only run it for one or two epochs. **[10 marks]** \n",
    "\n",
    "**Tip for efficiency:** *when developing your model, try training and testing the model on one batch (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build model...\n",
      "training model 1...\n",
      "epoch 1, average loss: 0.9153\n",
      "epoch 2, average loss: 0.7668\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "embedding_dim = 64\n",
    "hidden_size = 32\n",
    "dropout_prob = 0.1\n",
    "lr = 0.0001\n",
    "\n",
    "# train_iter = dataset['train'].iter(batch_size=batch_size)\n",
    "print(\"build model...\")\n",
    "model = SNLIModel(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_size=hidden_size, num_classes=3, dropout_prob=dropout_prob)\n",
    "model.to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print(\"training model 1...\")\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    total_loss = 0.0 # reset\n",
    "\n",
    "    # train model\n",
    "    model.train()  # train mode\n",
    "    for batch in train_loader:\n",
    "        \n",
    "        # reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        premise = batch['premise']\n",
    "        hypothesis = batch['hypothesis']\n",
    "        labels = batch['label'] \n",
    "        \n",
    "        premise = premise.to(device)\n",
    "        hypothesis = hypothesis.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # print(\"premises:\",premise)\n",
    "        # print(\"labels:\",labels) # (batch_size, sequence_length)\n",
    "        \n",
    "        outputs = model(premise, hypothesis)\n",
    "        # print(\"outputs:\",outputs)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # accumulate losses\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # calculate average loss\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"epoch {epoch + 1}, average loss: {avg_loss:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing\n",
    "\n",
    "Test the model on the testset. For each example in the test set, compute a prediction from the model (`entailment`, `contradiction` or `neutral`). Compute precision, recall, and F1 score for each label. **[10 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            premise = batch['premise']\n",
    "            hypothesis = batch['hypothesis']\n",
    "            labels = batch['label']\n",
    "\n",
    "            premise = premise.to(device)\n",
    "            hypothesis = hypothesis.to(device)\n",
    "            labels = labels.to(device)\n",
    "      \n",
    "            outputs = model(premise, hypothesis)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Store labels and predictions for further evaluation\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average=None, labels=[0, 1, 2])\n",
    "    label_names = ['entailment', 'neutral', 'contradiction']\n",
    "    \n",
    "    # Compare indices\n",
    "    print(\"Compute precision, recall, and F1 score for each label:\")\n",
    "    for i, label in enumerate(label_names):\n",
    "        print(f\"Label: {label}\")\n",
    "        print(f\"Precision: {precision[i]}\")\n",
    "        print(f\"Recall: {recall[i]}\")\n",
    "        print(f\"F1 Score: {f1[i]}\")\n",
    "        print()\n",
    "\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute precision, recall, and F1 score for each label:\n",
      "Label: entailment\n",
      "Precision: 0.7432311811960726\n",
      "Recall: 0.7416864608076009\n",
      "F1 Score: 0.742458017536038\n",
      "\n",
      "Label: neutral\n",
      "Precision: 0.621773288439955\n",
      "Recall: 0.688412550481516\n",
      "F1 Score: 0.6533982013858174\n",
      "\n",
      "Label: contradiction\n",
      "Precision: 0.7150741635046568\n",
      "Recall: 0.6404077849860983\n",
      "F1 Score: 0.675684485006519\n",
      "\n",
      "Total accuracy on test set: 69.09%\n"
     ]
    }
   ],
   "source": [
    "# test model after all epochs are completed\n",
    "accuracy, precision, recall, f1 = test_model(model, test_loader)\n",
    "print(f\"Total accuracy on test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest a _baseline_ that we can compare our model against **[2 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to suggest a **random guessing** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Guessing Accuracy: 0.3377442996742671\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def random_guessing(test_loader):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            labels = batch['label']\n",
    "\n",
    "            # Generate random predictions for each label\n",
    "            batch_size = len(labels)\n",
    "            random_predictions = [random.randint(0, 2) for _ in range(batch_size)]\n",
    "\n",
    "            # Store labels and predictions for further evaluation\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(random_predictions)\n",
    "\n",
    "    # Calculate accuracy using sklearn\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Computation\n",
    "random_accuracy = random_guessing(test_loader)\n",
    "print(f\"Random Guessing Accuracy: {random_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, it is appearant that our SNLI model perfoms better.  \n",
    "Actually, since the label distribution of the test set is close to a *uniform distribution*, the result of this baseline model is predictable, just around *1/n_classes*.  \n",
    "So if a model's accuracy is close to random guessing, it may not have learned useful patterns from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary labels: Counter({0: 3368, 2: 3237, 1: 3219})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"summary labels:\",Counter(test['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways (other than using a baseline) in which we can analyse the models performance **[3 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Confusion Matrix**  \n",
    "The confusion matrix compares the true labels with the labels predicted by the model and organizes the results into a matrix to show the relationship between various classification results. By analyzing the confusion matrix, we can understand which categories the model performs well and which categories it performs poorly on, and identify the types of errors that the model may have.\n",
    "- **ROC Curve**  \n",
    "The ROC curve shows the relationship between the true positive rate and the false positive rate. By analyzing the curve, we can better understand the model performance at different thresholds and choose the most appropriate threshold.\n",
    "- **Cross-Validation**  \n",
    "Using cross-validation can assess the model performance more accurately and test the generalization ability of the model on different data subsets. By using different training and validation data sets, we can reduce the bias caused by uneven or random distribution of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways to improve the model **[3 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to improve the model from these perspectives:  \n",
    "**Model Structure Adjustment**:  \n",
    "Here we just use one-layer Bi-LSTM with max-pooling, we can try more complex architectures like the *HBMP* model, or other architectures like *Transformer*.  \n",
    "**Hyperparameter Tuning**:  \n",
    "Limited by the computation resources this time, we just tried few hyperarameters. We can find an optimal combination of hyperparameters using grid search.   \n",
    "**Early stopping**:  \n",
    "Though we didn't introduce validation set here, to monitor the perfomance of each epoch and stop training when parameter updates no longer begin to yield improvement is a suggested way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings\n",
    "\n",
    "[1] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). \n",
    "\n",
    "[2] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks\n",
    "\n",
    "This assignment has a total of 23 marks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
